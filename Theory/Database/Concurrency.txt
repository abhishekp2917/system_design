*   What are various issues that may arise due to concurrent access to DB ?

>>  (1) Dirty Reads:

        - A dirty read occurs when a transaction reads data that has been modified by another transaction but not yet 
          committed. 
        
        - If the modifying transaction rolls back or fails, the data read by the first transaction becomes invalid, leading 
          to inconsistencies.

        Example:

            (a) Initial State:

                - Account A has a balance of $1,000. 

            (b) Transaction A:

                - Updates the balance of Account A to $500 in memory but does not commit the change yet.
            
            (c) Transaction B:

                - Reads the updated balance ($500) from Account A, believing it is correct.
                - Uses this value in its calculations or displays it to the user.
            
            (d) Transaction A:

                - Rolls back due to some issue, restoring the balance of Account A to $1,000.
            
            (e) Transaction B:

                - Now has an incorrect value ($500) for its operations because it read uncommitted data.

    (2) Dirty Writes:

        - A dirty write occurs when one transaction modifies data that another transaction has also modified but not yet 
          committed. 
        
        - If either transaction is rolled back, the changes made by the other transaction can become invalid, leading to 
          inconsistencies in the database.

        Example:

            - Two users attempting to update the balance of the same bank account 'A' concurrently which has a balance 
              of $1,000.

            (a) Transaction A:

                - Begins and subtracts $500 from Account A (temporary balance: $500) but does not commit.
            
            (b) Transaction B:

                - Begins and attempts to add $200 to Account A.
                - Writes the updated balance as $700, overwriting Transaction A's changes.

            (c) Transaction A:

                - Rolls back its transaction, leaving the balance at $700 instead of $1,200.
                - The database ends up with a corrupted state, as the changes were based on uncommitted data.

    (3) Non-Repeatable Reads:

        - A non-repeatable read occurs when a transaction reads the same data twice and gets different results because 
          another transaction modified or deleted the data in between the two reads. 
        
        - This inconsistency is caused by concurrent transactions and can lead to logical errors in applications.

        Example:

            - Two concurrent transactions are accessing and modifying a database that stores a product's price.

            (a) Initial State:

                - Product P has a price of $100.
            
            (b) Transaction A:

                - Starts and reads the price of P as $100.
            
            (c) Transaction B:

                - Starts, updates the price of P to $120, and commits.

            (d) Transaction A:

                - Reads the price of P again. But this time price is $120.
            
            - The price of P read by Transaction A is different in the second read, leading to a non-repeatable read.

    (4) Phantom Reads:

        - A phantom read happens when a transaction runs the same range query twice, but another transaction in between these 
          execution adds or removes rows that match the query, causing the result set to change.

        Example:

            (a) Initial State:

                - The Accounts table contains the following rows:
                    _________________________________
                    |   Account ID  |   Balance     |
                    |_______________|_______________|
                    |   1	        |   15,000      |
                    |   2	        |   12,000      |
                    |   3           |   14,000      |
                    |_______________|_______________|

            (b) Transaction A:

                - Starts and queries for accounts with a balance greater than $10,000.
                - The query result in Accounts 1, 2 and 3.

            (c) Transaction B:

                - Updates the Balance of Account 1 to $9,000, and commits.
                    _________________________________
                    |   Account ID  |   Balance     |
                    |_______________|_______________|
                    |   1	        |   9,000       |
                    |   2	        |   12,000      |
                    |   3           |   14,000      |
                    |_______________|_______________|

            (d) Transaction A:

                - Queries again for accounts with a balance greater than $10,000.
                - The same query this time returns only Accounts 2 and 3.
        
    (5) Lost Updates:

        - A lost update occurs when two or more transactions read the same data, make modifications based on the read 
          value, and then update the data, causing one of the updates to be overwritten without ever being acknowledged 
          or preserved. 
          
        - This happens because the updates are not isolated, and the transactions inadvertently overwrite each otherâ€™s 
          changes.

        Example:

            (a) Initial State:

                - Product stock: 100 units.

            (b) Transaction A:

                - Reads the stock value: 100.
                - Decreases stock by 20 units (intended update: stock = 80).
            
            (c) Transaction B:

                - Reads the same stock value: 100.
                - Decreases stock by 10 units (intended update: stock = 90).

            (d) Transaction A:

                - Updates stock to 80 and commits.

            (c) Transaction B:

                - Updates stock to 90 and commits.
                - The final stock is 90, and Transaction A's update is lost.

________________________________________________________________________________________________________________________

*   What are various Isolation Levels ?

>>  Database Isolation Levels specify the extent to which one transaction must be isolated from other transactions in 
    the presence of concurrent transactions. 

    They are crucial for maintaining data consistency in multi-user environments where concurrent transactions may occur.

    (1) Read Uncommitted Isolation Level:

        - In the Read Uncommitted, transactions are allowed to read uncommitted changes made by other transactions. 

        - This means a transaction can see changes made by other transactions that have not been committed yet, leading 
          to phenomena like dirty reads.
        
        - Concurrency issues left unsolved:

            (a) Dirty Reads
            (b) Dirty Writes
            (c) Repeatable Reads 
            (d) Phantom Reads 
            (e) Lost Updates
    
    (2) Read Committed Isolation Level:

        - In the Read Committed, transactions can only read data that has been committed by other transactions. 
        
        - Changes made by a transaction are not visible to other transactions until they are committed.

        - Concurrency issues solved:

            (a) Dirty Reads
            (b) Dirty Writes
        
        - Concurrency issues left unsolved: 
    
            (a) Repeatable Reads 
            (b) Phantom Reads
            (c) Lost Updates

    (3) Repeatable Read Isolation Level:

        - In the Repeatable Read, once a row is read, it is locked until the transaction commits or rolls back. 

        - Changes made by other transactions are not visible to the transaction until it completes.

        - Concurrency issues solved:

            (a) Dirty Reads
            (b) Dirty Writes
            (c) Repeatable Reads 
        
        - Concurrency issues left unsolved: 
    
            (a) Phantom Reads 
            (b) Lost Updates

    (4) Serializable Isolation Level:

        - The Serializable isolation level ensures that transactions are executed as if they were serialized, one after 
          another, by implementing range locks without overlapping access to shared resources.

        - Performance can be very slow because transactions need to wait for locks to be released, leading to potential 
          bottlenecks.

        - Suitable for systems where strict consistency is required, like financial transactions or critical databases.

        - Concurrency issues solved:

            (a) Dirty Reads
            (b) Dirty Writes
            (c) Repeatable Reads 
            (d) Phantom Reads 
            (e) Lost Updates

________________________________________________________________________________________________________________________

*   What is Optimistic Locking ?

>>  Optimistic locking assumes that conflicts between transactions are rare and instead of helding locks on the data 
    during the transaction, it focuses on detecting and resolving conflicts only when they occur.

    How It Works:

        (a) Read Phase: 
        
            - A transaction reads the data.
            - Instead of locking the data, it records a version number or a timestamp when the data is first read.

        (b) Processing Phase: 
        
            - The transaction performs its operations locally without locking the data.

        (c) Validation Phase: 
        
            - Before committing, the system compares the current version of the data in the DB with the recorded one.
            - If they match, it means no other transaction has modified the data, and the transaction can safely commit.
            - If they don't match, the transaction is rolled back and retried.

    Advantages:

        - High concurrency/performance because transactions do not block each other during execution.
        - Reduces lock overhead, making it suitable for read-heavy workloads.

    Disadvantages:

        - Transactions might need to be retried multiple times if conflicts occur.
        - Not suitable for systems with high contention and requiring strict consistency.

________________________________________________________________________________________________________________________

*   What is Pessimistic Locking ?

>>  Pessimistic locking assumes that conflicts are likely and prevents them by locking the data as soon as it is accessed.

    Locks are acquired on data (shared or exclusive) to prevent concurrent access during a transaction.

    How It Works:
        
        (a) Read Phase: 
        
            - When a transaction reads data, it acquires a lock (shared or exclusive).

        (b) Processing Phase: 
        
            - The lock prevents other transactions from modifying (or even reading, in the case of exclusive locks) the data.
        
        (c) Commit Phase: 
        
            - Once the transaction completes, the lock is released.

    Advantages:
        
        - Suitable for critical systems requiring strict consistency (e.g., financial transactions).

    Disadvantages:
        
        - Transactions must wait for locks to be released, reducing concurrency and performance.
        - Circular waiting for locks can lead to deadlocks.

________________________________________________________________________________________________________________________

*   Explain Shared Locks and Exclusive Locks ?

>>  (1) Shared Lock:
        
        - Allow multiple transactions to read the same data simultaneously while the lock is held.
        
        - Prevents other transactions from writing to the data until the lock is released.
        
        - Suitable for read-heavy systems where data is read frequently but modified less often.

    (2) Exclusive Lock:
        
        - Prevents other transactions from reading and writing to the data item until the lock is released.
        
        - Ensures that modifications made by one transaction are completed without interference.
        
        - Suitable for write-heavy systems where data updates are frequent.

________________________________________________________________________________________________________________________

