*   What is Indexing in Database ?

>>  Indexing in databases is a technique used to improve the speed of data retrieval operations (time complexity: log(n)) 
    on a table at the cost of additional storage space. 
    
    It works by creating a data structure (often a B-tree or hash index) that allows the database to quickly locate 
    rows based on the values in indexed columns, without scanning the entire table.

    Let for the given below table, index is applied in column 'last_name', then DB will create a B-tree of the data shown 
    below. 
            ________________________________
            | id  | first_name | last_name |
            |-----|------------|-----------|
            | 1   | John       | Allen     |
            | 2   | Jason      | Ford      |
            | 3   | Johnathan  | Roberts   |
            | 4   | Joseph     | Nelson    |
            | 5   | Jamie      | Moore     |
            | 6   | Jim        | Clark     |
            | 7   | Jeremy     | Quinn     |
            | 8   | Julie      | Green     |
            | 9   | Joan       | Smith     |
            | 10  | Jill       | Evans     |
            | 11  | Jake       | Davis     |
            | 12  | Jordan     | Taylor    |
            | 13  | Jerome     | Lee       |
            | 14  | Julia      | James     |
            | 15  | Jasmine    | Irwin     |
            | 16  | Jack       | Hill      |
            | 17  | Jane       | Brown     |
            | 18  | Janelle    | Olsen     |
            | 19  | Jacek      | King      |
            | 20  | Justin     | Park      |
            |_____|____________|___________|
    
    
                    _______________ [Evans, Lee] _________________________________________________________
                    |                      |                                                             |           
                    |                      |                                                             |          
                    |                      |                                                             |              
                    |                      |                                                             |  
        _________ [Clark]________       [Ford, Irwin] _______                                ______ [Quinn, Roberts] ___________
        |                       |           |               |                                |                                 |
        |                       |           |               |                                |                                 |
        |                       |           |               |                                |                                 |
        |                       |           |               |                                |                                 |      
    [Allen, Brown]           [Davis]  [Green, Hill]      [James, King]        _______ [Olsen, Park]                    [Smith, Taylor]
                                                                              |                                       
                                                                              |               
                                                                              |               
                                                                              |               
                                                                      [Moore, Nelson]

    Drawbacks of Indexing:
        
        (1) Increased Storage Requirements:

            Each index consumes additional disk space. In cases where there are many indexes, the storage cost can be 
            substantial.
            
            For very large databases, this can lead to significant space management concerns and increased storage 
            expenses.

        (2) Slower Write Operations:

            Insert, update, and delete operations become slower because the indexes need to be maintained and updated 
            whenever the data changes.

            Each write operation may involve updating not just the data but also the relevant index structures, which 
            can add overhead.
        
        (3) Complexity in Index Management:

            Maintaining a large number of indexes can make database management more complex. Determining which columns 
            to index and balancing performance can be challenging.

            Improper index design can lead to inefficient queries and increased maintenance tasks.

________________________________________________________________________________________________________________________

*   What are SSTables (Sorted String Tables) ?

>>  SSTables are immutable data files used in storage systems to store key-value pairs. The data in SSTables is sorted 
    by keys, which makes them efficient for lookups, range queries, and merging operations.

    When multiple SSTables are present, they may be merged or compacted periodically to maintain efficient read 
    performance and reduce the number of SSTables that need to be checked during a query.

    Assume a database is storing user information (e.g., user ID as the key and user details as the value). The data 
    in SSTables might look like this:

        Key	                Value
    _________________________________________
    |   1001    |	Alice, 25, New York     |
    |   1002	|    Bob, 30, Los Angeles   |
    |   1003	|    Charlie, 22, Miami     |
    _________________________________________

    In the SSTable, the index will map keys (1001, 1002, 1003) to their respective positions in the data block, 
    allowing for quick lookup.
        
________________________________________________________________________________________________________________________

*   What are LSM Trees (Log-Structured Merge Trees) ?

>>  An LSM tree is a data structure designed to handle a high volume of write operations efficiently. It organizes data 
    in a way that optimizes write throughput by writing new data sequentially to disk and deferring merges until later.

    How It Works:

    Let's take below data entries to understand it better:

        Key	                Value
    _________________________________________
    |   1001    |	 Alice, 25, New York    |
    |   1004	|    Bob, 30, Los Angeles   |
    |   1003	|    Charlie, 22, Miami     |
    |   1002	|    Kat, 35, Los Angeles   |
    _________________________________________

        (1) In-memory Component (Memtable):
            
            New data is first written to an in-memory table (memtable), which supports fast inserts and updates. 
            
            The data in the memtable is kept in a sorted order, typically implemented as a balanced tree 
            (often a Red-Black Tree), providing O(log N) time complexity for read/write.

                                1002 [Black]
                                /          \
                               /            \
                          1001 [Red]     1003 [Black]
                                               \
                                                \
                                            1004 [Red]


        (2) Flushing to Disk: 

            When the in-memory component reaches a certain size, it is flushed to disk as an immutable SSTable.

            SSTable1:
            _________________________________________
            |   1001    |	 Alice, 25, New York    |
            |   1004	|    Bob, 30, Los Angeles   |
            _________________________________________

            SSTable2:
            _________________________________________
            |   1002	|    Kat, 35, Los Angeles   |
            |   1003	|    Charlie, 22, Miami     |
            _________________________________________

        (3) Compaction:

            Over time, multiple SSTables (SSTable1, SSTable2, etc.) are created. A compaction process merges these 
            SSTables, removing duplicate entries and ensuring data remains sorted and efficient to read.

            Merged SSTable:
            _________________________________________
            |   1001    |	 Alice, 25, New York    |
            |   1002	|    Kat, 35, Los Angeles   |
            |   1003	|    Charlie, 22, Miami     |
            |   1004	|    Bob, 30, Los Angeles   |
            _________________________________________

________________________________________________________________________________________________________________________

*   How SSTables and LSM Trees are more write efficient than Indexing using B-Trees ?

>>  While SSTables and LSM Trees provide O(log N) time complexity for both read and write operations, similar to B-trees, 
    LSM Trees and SSTables more suitable for write-heavy workloads because:

    (1) In-memory writes:

        - In B-trees, insert/update require random writes to disk, which can significantly impact write performance, 
          especially when there's a large number of concurrent writes.

        - LSM Trees have a unique design where writes are first buffered in memory (in the memtable) and periodically 
          flushed to disk as SSTables. This reduces the need for frequent random writes to disk, which can be costly in 
          B-tree-based systems even if time complexity ib both case is O(log(n)).

    (2) Efficient Handling of Deletes and Updates:

        - STables and LSM Trees handle deletes and updates more efficiently. When a key is deleted or updated, it doesn't 
          require modifying the tree structure. Instead, the update or delete is just a new entry in the memtable or a 
          new SSTable. The obsolete data is eventually purged during the compaction process.

        - In contrast, B-trees require explicit modification of the tree structure (node splitting, rebalancing), which 
          can become more complex and resource-intensive, especially in systems with high rates of updates and deletes.

    (3) Compaction in LSM Trees vs. Rebalancing in B-trees:

        - Compaction in LSM trees happens in the background and can be done in bulk, so it's more efficient for 
          write-heavy workloads.

        - B-trees, however, require rebalancing after every insertion, deletion, or update. This rebalancing is not as 
          efficient for write-heavy workloads because it involves rearranging nodes, redistributing keys, and 
          maintaining the tree's balance after every operation.

________________________________________________________________________________________________________________________

*   What are the properties of SQL DBMS ?

>>  (1) Fixed Schema:

        SQL databases require a predefined schema, meaning that the structure of the data (tables, columns, data types, 
        and constraints) must be defined before data is inserted. This enforces data consistency and helps manage 
        complex data relationships.

    (2) ACID Compliance:

        (a) Atomicity: 
        
            Ensures that each transaction is all-or-nothing, so if one part of the transaction fails, the entire 
            transaction fails and the database remains unchanged.

        (b) Consistency: 
        
            SQL databases generally ensure strong consistency by committing transactions in a synchronous manner, 
            making sure that all reads reflect the most recent write operations.

        (c) Isolation: 
        
            Ensures that concurrent transactions do not interfere with each other and that the outcome is as if the 
            transactions were executed sequentially.

        (d) Durability: 
            
            Ensures that once a transaction is committed, it remains so, even in the case of a system failure.

    (3) Relational Structure:

        SQL databases store data in tables, with rows representing individual records and columns representing attributes. 
        
        Tables can have relationships with one another using foreign keys, enabling structured and normalized data storage.

    (4) Normalization:

        SQL DBMSs often use normalization techniques to minimize data redundancy and maintain data integrity.

    (5) Structured Query Language (SQL):

        SQL supports complex queries, joins, aggregations, subqueries, and analytical operations, making it versatile 
        for data retrieval and manipulation.

________________________________________________________________________________________________________________________

*   List out DBMS which leverages SSTables and LSM Trees ?

>>  Several NoSQL databases and distributed databases leverage SSTables and LSM Trees to manage data efficiently. 

    Databases That Leverage SSTables and LSM Trees:

        - Cassandra
        - HBase
        - LevelDB 

________________________________________________________________________________________________________________________

*   What are the properties of NoSQL DBMS ?

>>  (1) Schema Flexibility:

        Unlike SQL databases, NoSQL databases do not require a fixed schema, allowing data structures to be more flexible 
        and different records in the same collection/table can have different fields.

    (2) Performance:

        NoSQL databases are designed for high read/write throughput, making them ideal for applications that need to 
        handle large volumes of data and high traffic.

    (3) ACID vs BASE Properties:

        While most NoSQL databases do not fully adhere to ACID properties, they often follow BASE principles, focusing 
        on availability and eventual consistency to achieve high performance and scalability.

    (4) Support for Big Data:

        NoSQL databases are suited for applications that need to process and store vast amounts of unstructured or 
        semi-structured data and can handle real time data processing needs.

    (5) Scalability:

        NoSQL databases are typically designed to scale out by adding more servers to handle increased loads, rather than 
        scaling up with more powerful hardware.

    (5) High Availability and Fault Tolerance:

        Data is often replicated across multiple nodes to ensure availability in case of failures.

________________________________________________________________________________________________________________________

*   What are the disadvantages of NoSQL ?

>>  (1) Limited ACID Compliance:

        Although some NoSQL databases like MongoDB and Couchbase offer ACID-compliant transactions, full multi-document 
        or multi-node transaction support is less common and may come with trade-offs in performance and complexity.

    (2) Limited Query Capabilities:

        Some NoSQL databases may not support complex querying or joins as efficiently as SQL databases, requiring more 
        complicated application logic or multiple queries to achieve the same result.

    (3) Lack of Relationship Management:

        For applications that require strict enforcement of relationships between different entities 
        (e.g., maintaining referential integrity), NoSQL databases aren't suited.

________________________________________________________________________________________________________________________

*   How NoSQL DBMS are faster than SQL DBMS in read/write operation ?

>>  (1) Read operation in NoSQL is faster because:  

        (a) Simplified Data Model:

            NoSQL databases often use simpler data models (e.g., key-value pairs, documents, or graphs).

            In key-value stores (e.g., Redis, DynamoDB), data can be read/write in O(1) as each record has a unique 
            key to identify.

            Similarly, in document stores (e.g., MongoDB), each document in a collection has a unique id which makes 
            read/write operation in O(1) and doesn't require full scan of the colection.

            ------------------------------------------------------------------------------------------------------------
            Important! :
            
            If read query is based on field and not the id or the key, then even NoSQL DBMS will perform full scan 
            just like SQL.
            
            But still NoSQL will be faster compare to SQL because of some other factors mentioned below. 
            ------------------------------------------------------------------------------------------------------------

        (b) Schema Flexibility:

            NoSQL databases like MongoDB often store data in a denormalized format, where related data is embedded 
            within a single document. 
            
            This means that when reading data, the database can fetch all relevant information with a single read 
            operation, avoiding the need for complex joins that are typical in relational SQL databases.

        (c) Horizontal Scalability:

            Many NoSQL databases are designed with built-in horizontal scaling in mind, meaning they can distribute 
            data across multiple servers or nodes. 
            
            This allows for parallel reads from different nodes, which can lead to faster read performance when 
            querying large datasets.

    (2) Write operation in NoSQL is faster because: 

        (a) Schema Flexibility:

            NoSQL databases, such as MongoDB, do not enforce a strict schema. 
            
            This flexibility means that write operations don’t need to check against a pre-defined table schema, 
            allowing for faster insertions or updates without additional overhead.

        (b) Denormalized Data Model:

            In NoSQL, data is often stored in a denormalized format, which means that related information is embedded 
            in a single document. 
            
            This reduces the need for multiple write operations across different tables or collections

        (c) Eventual Consistency:

            Many NoSQL databases, such as Cassandra, offer eventual consistency as opposed to the strict ACID 
            (Atomicity, Consistency, Isolation, Durability) properties enforced by SQL databases. 
            
            This relaxed consistency model allows write operations to be distributed quickly across nodes without 
            waiting for synchronous replication, leading to faster writes.

        (d) Horizontal Scalability:

            NoSQL databases are designed for horizontal scaling, meaning data can be written across multiple nodes or 
            servers. 
            
            This allows for high-throughput write operations as the load is distributed.

________________________________________________________________________________________________________________________

*   How NoSQL can be scaled horizontally more easily compare to SQL ?

>>  (1) Data Model Flexibility:

        NoSQL has flexible, schema-less data models, which allow data to be partitioned and stored across different 
        nodes without complex relationships between tables.

        In document-based NoSQL databases like MongoDB, each document is often independent and self-contained, meaning 
        it does not rely on other documents for its integrity. This makes it simpler to distribute data across nodes 
        compared to the relational data model, where relationships between tables need to be maintained during scaling.

    (2) Eventual Consistency:

        Many NoSQL databases prioritize Availability and Partition Tolerance over strict consistency, which simplifies 
        scaling across nodes.

        Databases like Cassandra operate on an eventual consistency model, where data is replicated asynchronously 
        across nodes.

________________________________________________________________________________________________________________________

*   When to select SQL over NoSQL and viceversa ?

>>  When to Choose SQL:

        (1) Well-Defined Schema: 
        
            If your data has a well-defined schema and relationships that are unlikely to change frequently, SQL is ideal.

        (2) Complex Queries: 
            
            SQL excels at complex querying and supports joins, subqueries, and aggregations, making it suitable for 
            applications needing advanced analytics.

        (3) Strong Consistency:

            If your application requires strong consistency, where transactions must be fully ACID-compliant 
            (e.g., financial systems, banking applications, order management), SQL databases are designed to ensure this.

    When to Choose NoSQL:

        (1) Flexible Schema:

            If your application needs to handle semi-structured or unstructured data (e.g., JSON, XML) that might evolve 
            over time without significant schema changes, NoSQL is more suitable.

        (2) Scalability and High Availability:

            NoSQL databases are designed for horizontal scaling (distributing data across multiple nodes), making them 
            ideal for applications that need to scale out rather than up, such as social media platforms.

        (3) Eventual Consistency:

            If your application can handle eventual consistency in favor of improved availability and partition 
            tolerance, NoSQL databases provide that flexibility.

        (4) High Read/Write Throughput:

            NoSQL is optimized for high read/write throughput making it ideal for scenario where large number of 
            read/writes is required. 

________________________________________________________________________________________________________________________

*   What is Sharding (Horizontal Partitioning) ?

>>  Sharding is a database architecture pattern where data is horizontally partitioned across multiple databases, servers, 
    or nodes, referred to as shards. Each shard holds a subset of the data, and together all the shards form the complete 
    dataset. 
    
    Sharding is primarily used to handle large-scale applications where the dataset exceeds the capacity of a single 
    server, ensuring the system can scale horizontally to distribute the load. 

    It can also improve the read operation but not significantly. 

    Here’s how sharding typically works:

    (1) Sharding Key (Partition Key):

        A sharding key is used to determine how data will be distributed across the shards.
        
        The sharding key can be any attribute of the data, like user ID, geographic location, or product category. 

        The choice of a good sharding key is crucial for achieving even distribution and avoiding hotspots (where a 
        single shard gets too much traffic or data).

    (2) Data Distribution:

        Data is divided based on the sharding key, and each shard holds records corresponding to a specific subset of 
        key values.

        For example, if a system uses 'user_id' as the sharding key, user records with IDs from 1 to 1,000,000 could 
        be stored in Shard-1, from 1,000,001 to 2,000,000 in Shard-2, and so on.

    (3) Shard Mapping and Query Routing:

        A shard map or routing table is maintained, which helps the system determine which shard holds the required data.

        When a query is made, the application or middleware checks the shard map to determine which shard(s) contain 
        the required data.

        Platform like 'Vitess' for 'MySQL' and 'Mongos' for 'MongoDB' can be used for routing the request to right shard.
        These platform forms a layer of abstraction between application and database making it as if there is only one
        database which application is interacting with. 

________________________________________________________________________________________________________________________

*   What are the benefits and challenges of Sharding (Horizontal Partitioning) ?

>>  Benefits of Sharding:
        
        (1) Scalability: 
        
            As data grows, new shards can be added to distribute the load. This allows the system to scale horizontally, 
            which is particularly important for large, distributed applications.

        (2) Performance: 
        
            With sharding, data is distributed across multiple servers, so queries are processed in parallel, improving 
            performance, especially for read-heavy workloads.

    Challenges with Sharding:

        (1) Complexity: 
        
            Sharding introduces complexities in database management, such as the need for a shard map, data migration 
            when adding new shards, and managing cross-shard queries.

        (2) Data Distribution: 
        
            If the sharding key is not chosen wisely, it can lead to uneven data distribution, causing some shards to 
            become overloaded (also known as "hotspots").

        (3) Cross-Shard Queries: 
        
            Queries that need to access data across multiple shards (e.g., aggregations, joins) can be more complex and 
            slower than in non-sharded databases, as they may require querying multiple shards simultaneously and then 
            combining the results.

________________________________________________________________________________________________________________________

*   What is Vertical Partitioning ?

>>  Vertical partitioning is a database optimization technique in which the columns of a table are divided into separate, 
    smaller tables. Each smaller table includes a subset of columns from the original table, and they are typically 
    joined together by a common primary key or unique identifier.

    How Vertical Partitioning Works:

        Imagine an e-commerce database with a products table that contains the following columns:
            _________________________________________________________________________________________________
            |   ProductID	|   Name    |   Description |   Price   |   StockQuantity   |   SupplierDetails |
            _________________________________________________________________________________________________
        
        This table can be vertically partitioned into two separate tables:

            (1) Products_Info:
                
                Contains columns frequently accessed together, such as ProductID, Name, Description, and Price.
                _________________________________________________________
                |   ProductID   |   Name    |   Description |   Price   |
                _________________________________________________________
                
            (2) Products_Stock:
                
                Contains columns related to inventory or less frequently accessed data, such as ProductID, StockQuantity, 
                and SupplierDetails.
                _________________________________________________________
                |   ProductID   |   StockQuantity   |   SupplierDetails |
                _________________________________________________________

    Benefits of Vertical Partitioning:

        (1) Performance Improvement: 
        
            By separating frequently accessed columns from infrequently accessed ones, vertical partitioning can reduce 
            the amount of data read during queries, improving read performance.

        (2) Better Cache Utilization: 
        
            Smaller, more focused tables can fit more effectively into memory and caches, leading to faster data retrieval.

    Challenges of Vertical Partitioning:

        (1) Complexity of Joins: 
        
            Queries that need data from multiple partitions will require joins, potentially impacting performance.

        (2) Increased Complexity: 
        
            Database schema and application logic become more complex, as the system must handle multiple tables instead 
            of one.

        (3) Consistency Maintenance: 
        
            Ensuring data consistency across partitions can be more challenging, especially when updates involve columns 
            from different partitions.

________________________________________________________________________________________________________________________

*   How indexing work in Partitioned Database ?

>>  In database systems, local and global indexes are indexing strategies commonly used in Partitioned Databases to 
    optimize query performance.

    (1) Local Index:

        A local index is built within each individual partition of the database. It is specific to a single partition, 
        and the indexing scope is limited to the data in that partition. 

        Example :

            Consider an e-commerce database where Order Table is partitioned by CustomerID and Orders are indexed by 
            OrderDate within each of the partitions.

                Partition-1: Data for CustomerID 1-1000.
                ______________________________________________________________
                |   OrderID |   CustomerID  |   OrderAmount  |   OrderDate   |  
                |____________________________________________________________|
                |   1       |       60      |   4000.0       |  2024-01-01   |
                |   4       |       50      |   670.0        |  2024-01-03   |
                |   20      |       2       |   560.0        |  2024-01-15   |
                |   37      |       60      |   4190.0       |  2024-02-16   |
                |____________________________________________________________|

                Partition-2: Data for CustomerID 1001-2000.
                ______________________________________________________________
                |   OrderID |   CustomerID  |   OrderAmount  |   OrderDate   |  
                |____________________________________________________________|
                |   2       |       1002    |   666.0        |  2024-01-02   |
                |   3       |       1500    |   9199.0       |  2024-01-02   |
                |   11      |       1060    |   4450.0       |  2024-01-04   |
                |   40      |       1500    |   1980.0       |  2024-02-17   |
                |____________________________________________________________|
                
            If you query for all orders by CustomerID = 1500 and OrderDate > '2024-01-01', the query will only use 
            the local index in Partition-2 because all the data for CustomerID = 1500 resides in Partition-2.

        Benefits:

            - Faster for queries that are limited to a specific partition.

        Drawbacks:

            - Queries that span multiple partitions may require additional effort to combine results, as no single 
              index covers all partitions.

    (2) Global Index:

        A global index is built across all partitions of the database. It provides a unified indexing structure, 
        enabling queries to find data across multiple partitions.

        Example:
            
            Using the same e-commerce database, a global index could be built on OrderDate.

                All orders from all partitions are indexed by OrderDate:
                ______________________________________________________________
                |   OrderID |   CustomerID  |   OrderAmount  |   OrderDate   |  
                |____________________________________________________________|
                |   1       |       60      |   4000.0       |  2024-01-01   |
                |   2       |       1002    |   666.0        |  2024-01-02   |
                |   3       |       1500    |   9199.0       |  2024-01-02   |
                |   4       |       50      |   670.0        |  2024-01-03   |
                |   11      |       1060    |   4450.0       |  2024-01-04   |
                |   20      |       2       |   560.0        |  2024-01-15   |
                |   37      |       60      |   4190.0       |  2024-02-16   |
                |   40      |       1500    |   1980.0       |  2024-02-17   |
                |____________________________________________________________|


            If you query for all orders placed after '2024-01-01', the global index can directly locate the relevant 
            data across all partitions, avoiding the need to scan each one individually.

        Benefits:

            - Enhances performance for read operation that span accross partition.
            
        Drawbacks:

            - Slow on write-heavy workloads because need to write to each partition to update the global index.
            - Adds up complexity by requiring coordination between partitions to keep the global index up to date.
            
________________________________________________________________________________________________________________________

*   What are various technologies for Shard management ? 

>>  Below are various technologies and tools used for query routing and shard management:

    (1) Vitess:
        
        - Open-source sharding middleware for MySQL.
        
        - Vitess abstracts the sharding logic, automatically routing queries to the appropriate shard based on the 
          shard key.
        
        - Supports horizontal scaling with sharding and can manage a large number of database instances.
                    

    (2) ProxySQL:

        - High-performance MySQL proxy.
        
        - Can route SQL queries to different MySQL servers based on rules.

        - Ideal for MySQL and MariaDB environments requiring load balancing and query routing.
        
        - Distributes read and write queries to separate database instances to optimize performance.

        - Reduces latency by caching results.

    (3) Citus:

        - Open-source extension to PostgreSQL for distributed SQL.
        
        - Routes and distributes queries across multiple PostgreSQL nodes.

        - Automatically shards tables and rebalances data as nodes are added or removed.
        
        - Improves performance by running parts of a query on different nodes concurrently.

    (4) MongoDB Sharding:

        - NoSQL database with built-in sharding support.
        
        - MongoDB uses a component called mongos to route queries to the appropriate shard based on 
          the shard key.

        - Handles data distribution, balancing, and replica management.

________________________________________________________________________________________________________________________

*   What are various technologies that provide support for load balancing database replicas ?

>>  Here is a list of technologies that provide support for load balancing database replicas:

    (1) Database Proxy Solutions
    
        (a) ProxySQL:

            Description: A high-performance, open-source proxy for MySQL and MariaDB.
            
            Features: Read/write splitting, query routing, load balancing across replicas, failover support, and query caching.
            
            Use Case: Ideal for MySQL and MariaDB users needing dynamic query routing and high availability.
        
        (b) HAProxy:

            Description: A reliable, open-source load balancer that supports HTTP, TCP, and database traffic.
            
            Features: Configurable for load balancing read and write operations, health checks, and failover.
            
            Use Case: Can be used for balancing traffic across multiple database replicas and nodes.

        (c) Nginx:

            Description: A web server that also acts as a reverse proxy and load balancer.
            
            Features: Can be configured for database load balancing and distributing connections across replicas.
            
            Use Case: Suitable for applications that need load balancing for lightweight database requests.

    (2) Database-Specific Load Balancing Solutions

        (a) Vitess:

            Description: An open-source sharding middleware for scaling MySQL.
            
            Features: Transparent sharding, read/write splitting, load balancing across replicas, and query aggregation.
            
            Use Case: Used in large-scale MySQL deployments for horizontal scalability and load management.

        (b) MongoDB (mongos):

            Description: A query router component used in MongoDB sharded clusters.
            
            Features: Routes read and write queries to appropriate shards and supports load distribution among replicas.
            
            Use Case: Best for MongoDB clusters that need to balance read queries across secondary nodes.
        
        (c) Citus:

            Description: An extension for PostgreSQL that enables distributed database capabilities.
            
            Features: Load balancing across worker nodes, parallel query execution, and data redistribution.
            
            Use Case: For PostgreSQL users looking for distributed and horizontally scalable database solutions.

________________________________________________________________________________________________________________________

*   What is WAL (Write-Ahead Logging) ?

>>  WAL is a logging mechanism that records changes to data in a log file in a sequential fashion before the changes are 
    committed to the main database. 
    
    This ensures that even if a failure occurs during a write operation, the system can recover and maintain a consistent 
    state by replaying or rolling back changes from the log.

    How WAL Works:

        (1) Transaction Initiation: 
        
            When a transaction starts making changes (e.g., inserting or updating records), these changes are not 
            immediately written to the database files.

        (2) Log Entry Creation: 
        
            Instead, they are first recorded in a log file called the WAL. This log contains details about the data 
            changes, including both before and after values.

        (3) Commit Operation: 
        
            Once the changes are safely written to the WAL, the transaction is considered committed. This ensures 
            that the data can be recovered even if the database crashes immediately after committing.

        (4) Flushing to Disk: 
        
            The WAL is stored on disk, providing durability because it persists beyond a system crash.

        (5) Applying Changes: 
        
            Periodically, the changes logged in the WAL are applied to the main database files in a process known as 
            checkpointing.

        (6) Crash Recovery: 
        
            If a crash occurs before the database files are updated, the WAL is used to replay the changes and bring 
            the database to a consistent state.

        (7) WAL Truncation:

            After a successful checkpoint, the portions of the WAL that are no longer needed for recovery (i.e., those 
            that have already been applied to the main database) can be truncated or deleted.

            This means that the database maintains only the recent history in the WAL.

            By truncating old parts of the WAL, the database minimizes the size of the log and speeding up the recovery 
            process through checkpointing.

    This functionality comes built-in with most modern relational and NoSQL database systems. It is an integral part of 
    the database's transaction management and recovery system to ensure ACID compliance.

    PostgreSQL, MySQL, Oracle Database, SQL Serve, MongoDB, Cassandra etc has these feature built-in.
    
________________________________________________________________________________________________________________________